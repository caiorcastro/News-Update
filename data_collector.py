#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CAMADA 1: Coleta e Armazenamento Bruto de Dados Esportivos
Sistema que coleta TUDO que seja relevante e salva separadamente
ExecuÃ§Ã£o: DiÃ¡ria Ã s 9h30 via agendamento
"""

import json
import os
from datetime import datetime, timedelta
import logging
import argparse
import pytz
from pathlib import Path

# Importar mÃ³dulos existentes
from real_sports_data import (
    get_today_events, get_tomorrow_events, get_recent_results, 
    get_esports_events, get_weekly_schedule
)
from real_news_scraper import get_sports_news

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/data_collector.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DataCollectorBruto:
    """Coleta bruta de dados esportivos com armazenamento estruturado"""
    
    def __init__(self, data_dir='data/bruto'):
        """Inicializa o coletor de dados brutos"""
        self.data_dir = Path(data_dir)
        self.timezone = pytz.timezone('America/Sao_Paulo')
        self.timestamp = datetime.now(self.timezone)
        
        # Criar diretÃ³rios
        self.setup_directories()
        
    def setup_directories(self):
        """Cria estrutura de diretÃ³rios necessÃ¡ria"""
        directories = [
            self.data_dir,
            self.data_dir / 'daily',
            self.data_dir / 'archive',
            Path('logs')
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
            
        logger.info(f"DiretÃ³rios criados: {self.data_dir}")
        
    def get_filename(self, data_type):
        """Gera nome do arquivo baseado no tipo e data"""
        date_str = self.timestamp.strftime('%Y%m%d')
        return self.data_dir / 'daily' / f"{data_type}_{date_str}.json"
    
    def save_data(self, data, filename, description):
        """Salva dados em arquivo JSON com metadados"""
        try:
            output = {
                'metadata': {
                    'timestamp': self.timestamp.isoformat(),
                    'data_type': description,
                    'total_items': len(data) if isinstance(data, list) else 1,
                    'collection_success': True,
                    'source': 'data_collector_bruto'
                },
                'data': data
            }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(output, f, ensure_ascii=False, indent=2)
                
            logger.info(f"âœ… {description}: {len(data) if isinstance(data, list) else 1} itens salvos em {filename}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Erro ao salvar {description}: {e}")
            return False
    
    def collect_sports_events(self):
        """Coleta eventos esportivos de hoje e amanhÃ£"""
        logger.info("ðŸ† Coletando eventos esportivos...")
        
        # Eventos de hoje
        today_events = get_today_events()
        filename_today = self.get_filename('eventos_hoje')
        self.save_data(today_events, filename_today, 'Eventos de Hoje')
        
        # Eventos de amanhÃ£
        tomorrow_events = get_tomorrow_events()
        filename_tomorrow = self.get_filename('eventos_amanha')
        self.save_data(tomorrow_events, filename_tomorrow, 'Eventos de AmanhÃ£')
        
        # Consolidar eventos prÃ³ximas 24h
        all_events = today_events + tomorrow_events
        filename_24h = self.get_filename('jogos_proximas_24h')
        self.save_data(all_events, filename_24h, 'Jogos PrÃ³ximas 24h')
        
        return {
            'today': len(today_events),
            'tomorrow': len(tomorrow_events),
            'total_24h': len(all_events)
        }
    
    def collect_recent_results(self):
        """Coleta resultados recentes"""
        logger.info("ðŸ“Š Coletando resultados recentes...")
        
        recent_results = get_recent_results()
        filename = self.get_filename('resultados_recentes')
        self.save_data(recent_results, filename, 'Resultados Recentes')
        
        return len(recent_results)
    
    def collect_esports_events(self):
        """Coleta eventos de e-sports"""
        logger.info("ðŸŽ® Coletando eventos de e-sports...")
        
        esports_events = get_esports_events()
        filename = self.get_filename('eventos_esports')
        self.save_data(esports_events, filename, 'Eventos E-sports')
        
        return len(esports_events)
    
    def collect_weekly_schedule(self):
        """Coleta programaÃ§Ã£o semanal"""
        logger.info("ðŸ“… Coletando programaÃ§Ã£o semanal...")
        
        weekly_schedule = get_weekly_schedule()
        filename = self.get_filename('programacao_semanal')
        self.save_data(weekly_schedule, filename, 'ProgramaÃ§Ã£o Semanal')
        
        return len(weekly_schedule)
    
    def collect_news(self):
        """Coleta notÃ­cias esportivas brutas"""
        logger.info("ðŸ“° Coletando notÃ­cias esportivas...")
        
        # Coletar notÃ­cias das Ãºltimas 24h
        news_data = get_sports_news()
        
        # Adicionar timestamp para cada notÃ­cia
        for news in news_data:
            news['collected_at'] = self.timestamp.isoformat()
            news['collection_source'] = 'data_collector_bruto'
        
        filename = self.get_filename('noticias_brutas_ultimas_24h')
        self.save_data(news_data, filename, 'NotÃ­cias Brutas Ãšltimas 24h')
        
        return len(news_data)
    
    def collect_market_data(self):
        """Coleta dados de mercado e transferÃªncias"""
        logger.info("ðŸ’° Coletando dados de mercado...")
        
        # Por agora, usar dados de notÃ­cias filtradas por mercado
        all_news = get_sports_news()
        market_news = [
            news for news in all_news 
            if any(keyword in news.get('title', '').lower() + news.get('description', '').lower() 
                  for keyword in ['transferÃªncia', 'contrataÃ§Ã£o', 'venda', 'mercado', 'reforÃ§o'])
        ]
        
        filename = self.get_filename('dados_mercado')
        self.save_data(market_news, filename, 'Dados de Mercado')
        
        return len(market_news)
    
    def collect_trending_topics(self):
        """Coleta tÃ³picos em alta baseado em notÃ­cias"""
        logger.info("ðŸ“ˆ Analisando tÃ³picos em alta...")
        
        all_news = get_sports_news()
        
        # Extrair palavras-chave mais frequentes
        keyword_count = {}
        trending_keywords = [
            'flamengo', 'palmeiras', 'corinthians', 'sÃ£o paulo', 'santos',
            'vasco', 'botafogo', 'fluminense', 'brasileirÃ£o', 'libertadores',
            'copa do brasil', 'seleÃ§Ã£o', 'neymar', 'lesÃ£o', 'tÃ©cnico'
        ]
        
        for news in all_news:
            text = f"{news.get('title', '')} {news.get('description', '')}".lower()
            for keyword in trending_keywords:
                if keyword in text:
                    keyword_count[keyword] = keyword_count.get(keyword, 0) + 1
        
        # Ordenar por frequÃªncia
        trending = sorted(keyword_count.items(), key=lambda x: x[1], reverse=True)[:10]
        
        trending_data = {
            'period': '24h',
            'trending_keywords': [{'keyword': k, 'mentions': v} for k, v in trending],
            'related_news': [
                news for news in all_news 
                if any(keyword[0] in f"{news.get('title', '')} {news.get('description', '')}".lower() 
                      for keyword in trending[:5])
            ][:10]
        }
        
        filename = self.get_filename('topicos_trending')
        self.save_data(trending_data, filename, 'TÃ³picos em Alta')
        
        return len(trending_data['trending_keywords'])
    
    def archive_old_data(self, days_to_keep=7):
        """Arquiva dados antigos"""
        logger.info(f"ðŸ—„ï¸ Arquivando dados antigos (>{days_to_keep} dias)...")
        
        cutoff_date = self.timestamp - timedelta(days=days_to_keep)
        daily_dir = self.data_dir / 'daily'
        archive_dir = self.data_dir / 'archive'
        
        archived_count = 0
        
        for file_path in daily_dir.glob('*.json'):
            # Extrair data do nome do arquivo
            try:
                date_str = file_path.stem.split('_')[-1]  # Pegar Ãºltimo elemento (data)
                file_date = datetime.strptime(date_str, '%Y%m%d')
                file_date = self.timezone.localize(file_date)
                
                if file_date < cutoff_date:
                    # Mover para arquivo
                    archive_path = archive_dir / file_path.name
                    file_path.rename(archive_path)
                    archived_count += 1
                    logger.info(f"Arquivado: {file_path.name}")
                    
            except (ValueError, IndexError) as e:
                logger.warning(f"NÃ£o foi possÃ­vel processar arquivo {file_path.name}: {e}")
        
        logger.info(f"âœ… {archived_count} arquivos antigos arquivados")
        return archived_count
    
    def generate_collection_summary(self, stats):
        """Gera resumo da coleta"""
        summary = {
            'collection_timestamp': self.timestamp.isoformat(),
            'collection_date': self.timestamp.strftime('%Y-%m-%d'),
            'collection_time': self.timestamp.strftime('%H:%M:%S'),
            'timezone': str(self.timezone),
            'statistics': stats,
            'total_items_collected': sum(v for v in stats.values() if isinstance(v, int)),
            'collection_success': True
        }
        
        # Salvar resumo
        summary_file = self.data_dir / f"collection_summary_{self.timestamp.strftime('%Y%m%d')}.json"
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        return summary
    
    def run_full_collection(self):
        """Executa coleta completa de todos os dados"""
        logger.info("ðŸš€ Iniciando coleta bruta completa de dados esportivos")
        logger.info("=" * 60)
        
        start_time = datetime.now()
        stats = {}
        
        try:
            # Coletar eventos esportivos
            events_stats = self.collect_sports_events()
            stats.update(events_stats)
            
            # Coletar resultados recentes
            stats['resultados_recentes'] = self.collect_recent_results()
            
            # Coletar e-sports
            stats['eventos_esports'] = self.collect_esports_events()
            
            # Coletar programaÃ§Ã£o semanal
            stats['programacao_semanal'] = self.collect_weekly_schedule()
            
            # Coletar notÃ­cias
            stats['noticias_coletadas'] = self.collect_news()
            
            # Coletar dados de mercado
            stats['dados_mercado'] = self.collect_market_data()
            
            # Analisar tÃ³picos trending
            stats['topicos_trending'] = self.collect_trending_topics()
            
            # Arquivar dados antigos
            stats['arquivos_antigos'] = self.archive_old_data()
            
            # Gerar resumo
            summary = self.generate_collection_summary(stats)
            
            # Calcular tempo total
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            logger.info("=" * 60)
            logger.info("âœ… COLETA BRUTA CONCLUÃDA COM SUCESSO!")
            logger.info(f"â±ï¸ Tempo total: {duration:.2f} segundos")
            logger.info("ðŸ“Š Resumo da coleta:")
            
            for key, value in stats.items():
                if isinstance(value, int):
                    logger.info(f"   â€¢ {key.replace('_', ' ').title()}: {value}")
                elif isinstance(value, dict):
                    for subkey, subvalue in value.items():
                        logger.info(f"   â€¢ {subkey.replace('_', ' ').title()}: {subvalue}")
            
            logger.info(f"ðŸ“ Dados salvos em: {self.data_dir}")
            return summary
            
        except Exception as e:
            logger.error(f"âŒ Erro durante coleta: {e}")
            return None

def main():
    """FunÃ§Ã£o principal"""
    parser = argparse.ArgumentParser(description='Coleta bruta de dados esportivos')
    parser.add_argument('--mode', choices=['bruto', 'test'], default='bruto',
                       help='Modo de execuÃ§Ã£o')
    parser.add_argument('--data-dir', default='data/bruto',
                       help='DiretÃ³rio para salvar dados')
    
    args = parser.parse_args()
    
    if args.mode == 'test':
        print("ðŸ§ª Modo de teste - executando coleta de exemplo...")
        
    # Inicializar coletor
    collector = DataCollectorBruto(data_dir=args.data_dir)
    
    # Executar coleta completa
    summary = collector.run_full_collection()
    
    if summary:
        print(f"\nâœ… Coleta concluÃ­da! Total de itens: {summary['total_items_collected']}")
        print(f"ðŸ“ Dados salvos em: {args.data_dir}")
    else:
        print("\nâŒ Falha na coleta!")

if __name__ == "__main__":
    main() 